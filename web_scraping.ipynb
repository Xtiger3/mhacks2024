{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6247033b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 5305,
     "status": "error",
     "timestamp": 1688128208706,
     "user": {
      "displayName": "Joyesh Meshram",
      "userId": "08771931488029831946"
     },
     "user_tz": -330
    },
    "id": "6247033b",
    "outputId": "ec252733-4541-4566-f349-87e52d262a56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time  # For handling time-related functions\n",
    "import re  # For regular expressions\n",
    "import numpy as np  # For numerica/anaconda/envs/l operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from bs4 import BeautifulSoup  # For web scraping\n",
    "from selenium import webdriver  # For browser automation\n",
    "from selenium.webdriver.chrome.service import Service  # For configuring the ChromeDriver service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906e498a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page_source(url, delay=10):\n",
    "    # Configure Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Ignore certificate errors\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    \n",
    "    # Start the browser in maximized mode\n",
    "    options.add_argument('--start-maximized')\n",
    "\n",
    "    # Create a Chrome WebDriver instance with the specified options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Open the specified URL in the browser\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow time for the page to load (adjust delay as needed)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Get the page source using BeautifulSoup for parsing\n",
    "    page_source = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Close the WebDriver to release resources\n",
    "    driver.close()\n",
    "\n",
    "    # Return the parsed page source\n",
    "    return page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5b7b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_titles_URLs(page_source, first_page=False):\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Find all problems using BeautifulSoup\n",
    "    problem_elements = page_source.find_all('a', href=True, class_=[\n",
    "            'h-5 hover:text-blue-s dark:hover:text-dark-blue-s',\n",
    "            'h-5 hover:text-blue-s dark:hover:text-dark-blue-s opacity-60'\n",
    "        ])[start_index:]\n",
    "\n",
    "    # Extract text and 'href' attribute values from problem elements and store in a list\n",
    "    titles = [e.text for e in problem_elements]\n",
    "    problems_url = [e['href'] for e in problem_elements]\n",
    "    \n",
    "    # Return the list of titles\n",
    "    return titles, problems_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08095d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_acceptances_difficulties(page_source, first_page=False):\n",
    "    # Find all div elements with the specified class using BeautifulSoup\n",
    "    div_elements = page_source.find_all('div', class_='mx-2 flex items-center py-[11px]')\n",
    "\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Extract text from the span elements within the div elements and store in a list\n",
    "    items = [\n",
    "        span_element.text.strip()\n",
    "        for div_element in div_elements\n",
    "        for span_element in [div_element.find('span')]\n",
    "        if span_element\n",
    "    ]\n",
    "\n",
    "    # Separate the items into acceptances and difficulties lists\n",
    "    acceptances, difficulties = [], []\n",
    "    for item in items:\n",
    "        if item:\n",
    "            (acceptances if item.endswith('%') else difficulties).append(item)\n",
    "\n",
    "    # Return the lists of acceptances and difficulties\n",
    "    return acceptances[start_index:], difficulties[start_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3bf93e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_single_page_df(url, first_page=False):\n",
    "    # Get the page source for the specified URL\n",
    "    page_source = get_page_source(url)\n",
    "\n",
    "    # Extract titles, problem URLs, acceptances, and difficulties from the page source\n",
    "    titles, problems_url = get_titles_URLs(page_source, first_page)\n",
    "    acceptances, difficulties = get_acceptances_difficulties(page_source, first_page)\n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    data = {\n",
    "        'title': titles,\n",
    "        'problem_URL': problems_url,\n",
    "        'difficulty': difficulties\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame using the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6c4610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(start=1, end=100, file_name='scrape.csv'):\n",
    "    # Initialize an empty list to store DataFrames for each page\n",
    "    list_of_dfs = []\n",
    "\n",
    "    # Set the flag for the first page\n",
    "    first_page = True if start == 1 else False\n",
    "\n",
    "    # Iterate over the specified range of pages\n",
    "    for i in range(start, end + 1):\n",
    "        # Construct the URL for the current page\n",
    "        url = 'https://leetcode.com/problemset/all/?page=' + str(i)\n",
    "\n",
    "        # Get the DataFrame for the current page and append it to the list\n",
    "        df = get_single_page_df(url, first_page)\n",
    "        list_of_dfs.append(df)\n",
    "\n",
    "        # Update the first_page flag for subsequent pages\n",
    "        first_page = False\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(path_or_buf=file_name, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d21c0f",
   "metadata": {},
   "source": [
    "Running Web-Scraping Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620a4797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "50 50 50\n",
      "3 3 3\n"
     ]
    }
   ],
   "source": [
    "df = scrape(start=1, end=67, file_name='scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff93e2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['problem_URL'] = df['problem_URL'].apply(lambda x: f\"https://leetcode.com{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "335da442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>problem_URL</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Two Sum</td>\n",
       "      <td>https://leetcode.com/problems/two-sum</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Add Two Numbers</td>\n",
       "      <td>https://leetcode.com/problems/add-two-numbers</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. Longest Substring Without Repeating Characters</td>\n",
       "      <td>https://leetcode.com/problems/longest-substrin...</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. Median of Two Sorted Arrays</td>\n",
       "      <td>https://leetcode.com/problems/median-of-two-so...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. Longest Palindromic Substring</td>\n",
       "      <td>https://leetcode.com/problems/longest-palindro...</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>3299. Sum of Consecutive Subsequences</td>\n",
       "      <td>https://leetcode.com/problems/sum-of-consecuti...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>3300. Minimum Element After Replacement With D...</td>\n",
       "      <td>https://leetcode.com/problems/minimum-element-...</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>3301. Maximize the Total Height of Unique Towers</td>\n",
       "      <td>https://leetcode.com/problems/maximize-the-tot...</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>3302. Find the Lexicographically Smallest Vali...</td>\n",
       "      <td>https://leetcode.com/problems/find-the-lexicog...</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3302</th>\n",
       "      <td>3303. Find the Occurrence of First Almost Equa...</td>\n",
       "      <td>https://leetcode.com/problems/find-the-occurre...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                            1. Two Sum   \n",
       "1                                    2. Add Two Numbers   \n",
       "2     3. Longest Substring Without Repeating Characters   \n",
       "3                        4. Median of Two Sorted Arrays   \n",
       "4                      5. Longest Palindromic Substring   \n",
       "...                                                 ...   \n",
       "3298              3299. Sum of Consecutive Subsequences   \n",
       "3299  3300. Minimum Element After Replacement With D...   \n",
       "3300   3301. Maximize the Total Height of Unique Towers   \n",
       "3301  3302. Find the Lexicographically Smallest Vali...   \n",
       "3302  3303. Find the Occurrence of First Almost Equa...   \n",
       "\n",
       "                                            problem_URL difficulty  \n",
       "0                 https://leetcode.com/problems/two-sum       Easy  \n",
       "1         https://leetcode.com/problems/add-two-numbers     Medium  \n",
       "2     https://leetcode.com/problems/longest-substrin...     Medium  \n",
       "3     https://leetcode.com/problems/median-of-two-so...       Hard  \n",
       "4     https://leetcode.com/problems/longest-palindro...     Medium  \n",
       "...                                                 ...        ...  \n",
       "3298  https://leetcode.com/problems/sum-of-consecuti...       Hard  \n",
       "3299  https://leetcode.com/problems/minimum-element-...       Easy  \n",
       "3300  https://leetcode.com/problems/maximize-the-tot...     Medium  \n",
       "3301  https://leetcode.com/problems/find-the-lexicog...     Medium  \n",
       "3302  https://leetcode.com/problems/find-the-occurre...       Hard  \n",
       "\n",
       "[3303 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a38d99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_topic_tags(page_source):\n",
    "    # Initialize an empty list to store topic tags\n",
    "    topic_tags = []\n",
    "\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    topic_tag_elements = page_source.find_all('a',\n",
    "                 class_='mr-4 rounded-xl px-2 py-1 text-xs transition-colors text-label-2 dark:text-dark-label-2 hover:text-label-2 dark:hover:text-dark-label-2 bg-fill-3 dark:bg-dark-fill-3 hover:bg-fill-2 dark:hover:bg-dark-fill-2') \n",
    "\n",
    "    # Extract text content from each topic tag element and append to the list\n",
    "    for topic_tag_element in topic_tag_elements:\n",
    "        topic_tag = topic_tag_element.text\n",
    "        topic_tags.append(topic_tag)\n",
    "\n",
    "    # Join the list of topic tags into a comma-separated string\n",
    "    topic_tags_str = ', '.join(f\"'{item}'\" for item in topic_tags)\n",
    "\n",
    "    # Return the formatted string of topic tags\n",
    "    return topic_tags_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5aaea3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_similar_questions(page_source):\n",
    "    # Initialize an empty list to store similar questions\n",
    "    similar_questions = []\n",
    "\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    similar_question_elements = page_source.find_all('a', class_='text-sm font-medium transition-none text-label-1 dark:text-dark-label-1 hover:text-blue-s dark:hover:text-dark-blue-s')\n",
    "\n",
    "    # Extract text content from each similar question element and append to the list\n",
    "    for similar_question_element in similar_question_elements:\n",
    "        similar_question = similar_question_element.text\n",
    "        similar_questions.append(similar_question)\n",
    "\n",
    "    # Join the list of similar questions into a comma-separated string\n",
    "    similar_questions_str = ', '.join(f\"{item}\" for item in similar_questions)\n",
    "\n",
    "    # Return the formatted string of similar questions\n",
    "    return similar_questions_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c2ebcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_is_premium(page_source):\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    premium_element = page_source.find('div', class_='text-md mb-6 text-center text-label-2 dark:text-dark-label-2')\n",
    "\n",
    "    # Determine premium status based on the existence of the element\n",
    "    is_premium = 'True' if premium_element else 'False'\n",
    "\n",
    "    # Return the premium status\n",
    "    return is_premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85d9fcb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(df, file_name='scrape2.csv'):\n",
    "    # Extract links for the specified range from the DataFrame\n",
    "    links = df['problem_URL']\n",
    "\n",
    "    # Initialize an empty list to store DataFrames for each link\n",
    "    dfs = []\n",
    "    \n",
    "    # Configure Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')  # Ignore certificate errors\n",
    "    options.add_argument('--start-maximized')  # Start the browser in maximized mode\n",
    "\n",
    "    # Create a Chrome WebDriver instance with the specified service and options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Iterate over the links and scrape data\n",
    "    for link in links:\n",
    "        i = 0\n",
    "        \n",
    "        # Open the specified URL in the browser\n",
    "        driver.get(link)\n",
    "\n",
    "        # Allow time for the page to load (adjust delay as needed)\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Get the page source using BeautifulSoup for parsing\n",
    "        page_source = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Create a dictionary to store scraped data\n",
    "        data = {'is_premium': get_is_premium(page_source)}\n",
    "\n",
    "        # Check if the problem is not premium before scraping additional data\n",
    "        if data['is_premium'] == 'False':\n",
    "            # Update the data dictionary with additional scraped data\n",
    "            data.update({\n",
    "                'topic_tags': get_topic_tags(page_source),\n",
    "                'similar_questions': get_similar_questions(page_source)\n",
    "            })\n",
    "\n",
    "            # Create a DataFrame for the current link and append it to the list\n",
    "            df = pd.DataFrame(data, index=[i])\n",
    "            dfs.append(df)\n",
    "            i += 1\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    df.to_csv(path_or_buf=file_name, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a3d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape(df, file_name='problems.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ddf50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
